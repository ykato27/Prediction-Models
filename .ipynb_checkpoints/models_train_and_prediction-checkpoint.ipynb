{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('hondagiken': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3441a0903e35de921d46d2b135b5e1383de3b27058683bc82b2494657958cd8f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Part:0 本ノートブックの実行手順  \n",
    " - 事前準備\n",
    "     - 前処理後のデータ\n",
    "     - ファイル形式：csvファイル\n",
    "     - 格納先：../data/raw/　に格納(ディレクトリの変更可)\n",
    " - 実施内容\n",
    "     - Patr1: ライブラリのインポートです。 \n",
    "\n",
    "     - Part2: パラメータの設定を行います。\n",
    "       - ランダムシードの設定\n",
    "       - 目的変数のリスト（targets）を作成\n",
    "       - 構築する予測モデルの集合（models）を作成\n",
    "       - 予測モデルのパラメータを設定\n",
    "\n",
    "     - Part3: データを読み込みます。実施項目は下記です。\n",
    "       - データの保存先のパスやデータ名称を指定します（変更がある場合はここで指定してください）。\n",
    "       - カラム名に全角文字列が含まれるか判定を行います。\n",
    "       - カラム名に全角文字列が含まれた場合は、半角文字へ変更します。\n",
    "       - インプットデータの確認を行います。問題がある場合は、前処理を再度行ってください。\n",
    "         - Cell_typeのカラムが一列目にあるか確認\n",
    "         - 欠損値データの有無を確認\n",
    "         - 文字データの有無を確認\n",
    "         - ユニーク数が1つのカラムが無いか確認\n",
    "       - 目的変数のリスト（targets）に含まれない目的変数を削除\n",
    "       - (対数処理を施す)\n",
    "\n",
    "     \n",
    "     - Part4: データの正規化・標準化を目的変数別に実施します。\n",
    "     \n",
    "     - Part5: データを学習データと評価データに目的変数別に分割します。Cell_type毎に比率が等しくなるように実施します。  \n",
    "     \n",
    "     - Part6: 予測モデルの学習を行います。各種モデルのパラメータはPart3で設定した通りに実行されます。  \n",
    "       - 実行結果は/models/実行時の日付時刻/tarined/目的変数名/ に'実行時の日付\\_モデル名\\_目的変数名.pickle'という名前で保存されます  \n",
    "    \n",
    "     - Part7: 分析条件（実験条件）をjson型式のファイルに保存します。\n",
    "     \n",
    "     - Part8: 評価データに対して予測を行います。\n",
    "       - 実行結果は/models/実行時の日付時刻/predicts/目的変数名/ に'実行時の日付\\_モデル名\\_目的変数名.csv'という名前で保存されます \n",
    "     \n",
    "     - Part9: 目的変数別に各モデルの評価指標を算出します。\n",
    "     \n",
    "     - Part10: 構築したモデルの変数重要度、回帰係数、yyプロット、残差プロットを出力します。\n",
    "       - 実行結果は/models/実行時の日付時刻/feature_analysis_plot/目的変数名/ に'実行時の日付\\_モデル名\\_目的変数名.png'という名前で保存されます    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part:1 ライブラリのインポート"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import pickle\n",
    "import pytz\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可視化用ライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# 標準化を行うライブラリ\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# データを学習・評価データへ分割するライブラリ\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# スコア計算ライブラリ\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# エラーを発生させるライブラリ\n",
    "from numpy.linalg import LinAlgError"
   ]
  },
  {
   "source": [
    "# Part:2 諸設定\n",
    " ここでは以下の設定と処理を行います。\n",
    " - 対象となる目的変数をリストの形式でtargetsに保存してください。\n",
    " - 予測モデルの学習・検証を行う対象のモデルを集合の形式でmodelsに保存してください。\n",
    " - 実験結果を保存するフォルダを/models/以下に'yyyymmddhhmm'の形式で作成します。\n",
    " - 元のデータセットから今回対象となる目的変数以外の目的変数のカラムを削除します。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムシードの値を設定してください。\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 学習データと評価データを分割するときの、評価データの割合を設定してください。\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# 使用する目的変数を指定してください。\n",
    "targets = ['T_2', 'T_12', 'T_1635']\n",
    "\n",
    "# モデルは次の9種から選んでください文字列は大文字小文字など例と同じ書き方でお願いします。\n",
    "# 「\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"GBDT\", \"AdaBoost\", \"RandomForest\", \"SVR\", \"MLP\"」\n",
    "models = [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"GBDT\", \"AdaBoost\", \"RandomForest\", \"SVR\", \"MLP\"]\n",
    "\n",
    "# 対数変換処理の有無の記録(実際に処理を行う場合対数変換処理のマークダウンセルをコードセルに変更してコメントアウト解除し実行してもらう必要があります)\n",
    "# 対数変換処理を行う場合は実験条件に記録しておくためにscale='Log'と変更してください\n",
    "# 対数変換を行わない場合：scale = 'Linear', 対数変換を行う場合: scale = 'Log'.\n",
    "scale = 'Linear'\n",
    "\n",
    "# モデルのパラメータを設定してください。\n",
    "# ここに記載していなパラメータについてはscikit-learnの初期設定値が自動で適用されます。\n",
    "param_adaboost = {'n_estimators': 50, 'learning_rate': 1, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_elasticnet = {'alpha': 0.2, 'l1_ratio': 0.5, 'fit_intercept': True, 'normalize': False,\n",
    "                    'positive': False, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_gbdt = {'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 1.0, 'min_samples_split': 2,\n",
    "              'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.0, 'max_depth': 3,\n",
    "              'alpha': 0.9, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_lasso = {'alpha': 0.2, 'fit_intercept': True, 'normalize': False, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_linearregression = {'fit_intercept': True, 'normalize': False}\n",
    "\n",
    "param_mlp = {'hidden_layer_sizes': (100, ), 'activation': 'relu', 'solver': 'adam',\n",
    "             'alpha': 0.0001, 'learning_rate': 'constant', 'learning_rate_init': 0.001,\n",
    "             'max_iter': 200, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_randomforest = {'n_estimators': 100, 'max_depth': None, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_ridge = {'alpha': 100, 'fit_intercept': True, 'normalize': False, 'random_state': RANDOM_STATE}\n",
    "\n",
    "param_svr = {'kernel': 'rbf', 'degree': 3, 'gamma': 'scale', 'C': 10, 'epsilon': 0.1}\n",
    "\n",
    "model_param_dict = {'AdaBoost': param_adaboost, 'ElasticNet': param_elasticnet, 'GBDT': param_gbdt,\n",
    "                    'Lasso': param_lasso, 'LinearRegression': param_linearregression, 'MLP': param_mlp,\n",
    "                    'RandomForest': param_randomforest, 'Ridge': param_ridge, 'SVR': param_svr}"
   ]
  },
  {
   "source": [
    "#### モデルパラメータのフルオプション\n",
    "```\n",
    "# Adaboostのパラメータ\n",
    "param_adaboost = {'base_estimator': None, 'n_estimators': 50, 'learning_rate': 1.0,\n",
    "                  'loss': 'linear', 'random_state': None}\n",
    "\n",
    "# ElasticNet回帰のパラメータ\n",
    "param_elasticnet = {'alpha': 1.0, 'l1_ratio': 0.5, 'fit_intercept': True, 'normalize': False,\n",
    "                    'precompute': False, 'max_iter': 1000, 'copy_X': True, 'tol': 0.0001,\n",
    "                    'warm_start': False, 'positive': False, 'random_state': None, 'selection': 'cyclic'}\n",
    "\n",
    "# GBDTのパラメータ\n",
    "param_gbdt = {'loss': 'ls', 'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 1.0, 'criterion': 'friedman_mse', \n",
    "              'min_samples_split': 2, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.0, 'max_depth': 3, \n",
    "              'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'init': None, 'random_state': None, \n",
    "              'max_features': None, 'alpha': 0.9, 'verbose': 0, 'max_leaf_nodes': None, 'warm_start': False, \n",
    "              'presort': 'deprecated', 'validation_fraction': 0.1, 'n_iter_no_change': None, 'tol': 0.0001, 'ccp_alpha': 0.0}\n",
    "\n",
    "\n",
    "# Lasso回帰のパラメータ\n",
    "param_lasso = {'alpha': 1.0, 'fit_intercept': True, 'normalize': False, 'precompute': False,\n",
    "               'copy_X': True, 'max_iter': 1000, 'tol': 0.0001, 'warm_start': False,\n",
    "               'positive': False, 'random_state': None, 'selection': 'cyclic'}\n",
    "\n",
    "# 線形回帰のパラメータ\n",
    "param_linearregression = {'fit_intercept': True, 'normalize': False, 'copy_X': True, 'n_jobs': None}\n",
    "\n",
    "# ニューラルネットのパラメータ\n",
    "param_mlp = {'hidden_layer_sizes': (100, ), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0001,\n",
    "             'batch_size': 'auto', 'learning_rate': 'constant', 'learning_rate_init': 0.001,\n",
    "             'power_t': 0.5, 'max_iter': 200, 'shuffle': True, 'random_state': None, 'tol': 0.0001,\n",
    "             'verbose': False, 'warm_start': False, 'momentum': 0.9, 'nesterovs_momentum': True,\n",
    "             'early_stopping': False, 'validation_fraction': 0.1, 'beta_1': 0.9, 'beta_2': 0.999,\n",
    "             'epsilon': 1e-08, 'n_iter_no_change': 10, 'max_fun': 15000}\n",
    "\n",
    "# ランダムフォレストのパラメータ\n",
    "param_randomforest = {'n_estimators': 100, 'criterion': 'mse', 'max_depth': None,\n",
    "                      'min_samples_split': 2, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.0,\n",
    "                      'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0,\n",
    "                      'min_impurity_split': None, 'bootstrap': True, 'oob_score': False, 'n_jobs': None,\n",
    "                      'random_state': None, 'verbose': 0, 'warm_start': False, 'ccp_alpha': 0.0, 'max_samples': None}\n",
    "\n",
    "# Ridge回帰のパラメータ\n",
    "param_ridge = {'alpha': 1.0, 'fit_intercept': True, 'normalize': False, 'copy_X': True,\n",
    "               'max_iter': None, 'tol': 0.001, 'solver': 'auto', 'random_state': None}\n",
    "\n",
    "# SVMのパラメータ\n",
    "param_svr = {'kernel': 'rbf', 'degree': 3, 'gamma': 'scale', 'coef0': 0.0, 'tol': 0.001,\n",
    "             'C': 1.0, 'epsilon': 0.1, 'shrinking': True, 'cache_size': 200, 'verbose': False, 'max_iter': -1}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part:3 データの読み込み  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測モデルの学習結果を保存するディレクトリを作成.\n",
    "\n",
    "# フォルダの名前は'yyyymmdd時刻'の形式でつくられます\n",
    "# 例) 2020年10月15日9時24分に実行した場合、folder_name = '202010150924' になります\n",
    "date_time = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
    "folder_name = date_time.strftime('%Y%m%d%H%M')\n",
    "\n",
    "# フォルダは ../models の配下に作成します\n",
    "if not os.path.exists(f'../models/{folder_name}'):\n",
    "    os.makedirs(f'../models/{folder_name}')\n",
    "\n",
    "# 入力データのディレクトリと学習結果などを出力するディレクトリを定義\n",
    "input_dir = '../data/raw/'\n",
    "output_dir = f'../models/{folder_name}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../models/202011271803/input_data/preprocessed_df.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# 以下にあるcsvデータを読み込みます\n",
    "input_file_name = 'preprocessed_df.csv'\n",
    "df = pd.read_csv(join(input_dir, input_file_name), encoding=\"shift_jis\")\n",
    "\n",
    "# 入力データを保存します\n",
    "input_file_dir_path = f'{output_dir}/input_data'\n",
    "if not os.path.exists(input_file_dir_path):\n",
    "    os.makedirs(input_file_dir_path)\n",
    "shutil.copy(join(input_dir, input_file_name), f'{input_file_dir_path}/{input_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zenkaku_column_exists(data):\n",
    "    \"\"\"カラム名に全角文字列が含まれているか確認を行う。\n",
    "\n",
    "    カラム名に全角文字列が含まれるか確認し、結果に対して以下の文章を表示する。\n",
    "\n",
    "    - 全角文字列を含む場合\n",
    "      ⇒ 全角文字が含まれています。\n",
    "    \n",
    "    - 全角文字列を含まない場合\n",
    "      ⇒ 全角文字は含まれていません。\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): 確認するデータフレーム.\n",
    "\n",
    "    \"\"\"\n",
    "    ZENKAKU_ls = [chr(0xff01 + i) for i in range(94)]\n",
    "    columns_name = \"\".join(data.columns)\n",
    "    for m in columns_name:\n",
    "        if m in ZENKAKU_ls:\n",
    "            print('全角文字が含まれています。')\n",
    "            return\n",
    "\n",
    "    print('全角文字は含まれていません。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "全角文字は含まれていません。\n"
     ]
    }
   ],
   "source": [
    "zenkaku_column_exists(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_zenkaku_to_hankaku(data):\n",
    "    \"\"\"カラム名の全角文字を半角文字へ変換する.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象とするデータ.\n",
    "\n",
    "    Returns:\n",
    "        output_df (pd.DataFrame): カラム名が半角になっているデータ.\n",
    "\n",
    "    \"\"\"\n",
    "    ZEN = \"\".join(chr(0xff01 + i) for i in range(94))\n",
    "    HAN = \"\".join(chr(0x21 + i) for i in range(94))\n",
    "    ZEN_to_HAN = str.maketrans(ZEN, HAN)\n",
    "\n",
    "    output_df = data.copy()\n",
    "    for column in output_df.columns:\n",
    "        if not set(column).isdisjoint(set(ZEN)):\n",
    "            output_df = output_df.rename(columns={column: column.translate(ZEN_to_HAN)})\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "全角文字は含まれていません。\n"
     ]
    }
   ],
   "source": [
    "# 全角のカラム名が含まれている際に半角に修正します\n",
    "df = transform_zenkaku_to_hankaku(df)\n",
    "\n",
    "zenkaku_column_exists(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_models(models):\n",
    "    \"\"\"modelsの入力が正しいか確認する\n",
    "    \n",
    "    - 想定していないモデル（ex. 決定木モデル）が記入された際にエラーが表示されるか\n",
    "    - 大文字・小文字の記載が間違っている際（ex. 正解：GBDT、入力：Gbdt）にエラーが表示されるか\n",
    "    - modelsに何も記入しなかった際に学習するモデルが選択されていないのでエラーが表示されるか\n",
    "    \n",
    "    Args:\n",
    "        models (list): 用いる機械学習モデルの名称のリスト\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(models) == 0:\n",
    "        raise ValueError(\"modelsが空です。\")\n",
    "\n",
    "    correct_model_ls = [\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\",\n",
    "                        \"GBDT\", \"AdaBoost\", \"RandomForest\", \"SVR\", \"MLP\"]\n",
    "    \n",
    "    check_models = list(set(models) - set(correct_model_ls))\n",
    "    if len(check_models) >= 1:\n",
    "        raise ValueError(f\"不適切なモデル名がmodelsに入力されています。\\n不適当な入力: {check_models}\")\n",
    "\n",
    "\n",
    "def check_null(df):\n",
    "    \"\"\"欠損値の有無を確認する\n",
    "    \n",
    "    - 説明変数に欠損値がある場合、エラーが発生するか\n",
    "    - 目的変数に欠損値がある場合、エラーが発生しないか\n",
    "    - Cell_typeに欠損値がある場合、エラーが発生するか\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "    \n",
    "    \"\"\"\n",
    "    target_col_ls = [col for col in df.columns if 'T_' in col]\n",
    "    df_feature = df.drop(target_col_ls, axis=1)\n",
    "    null_columns = list(df_feature.columns[df_feature.isnull().any()])\n",
    "    \n",
    "    if len(null_columns) >= 1:\n",
    "        raise ValueError(f\"以下のカラムには欠損値が含まれています。\\n欠損値を含む変数名: {null_columns}\")\n",
    "\n",
    "\n",
    "def check_column_first(df):\n",
    "    \"\"\"データにCell_typeが含まれていることを確認する\n",
    "\n",
    "    - 一列目が間違っている場合にエラーが発生するか\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "\n",
    "    \"\"\"\n",
    "    if 'Cell_type' != df.columns[0]:\n",
    "        raise ValueError(f\"カラムの一列目に'Cell_type'がありません。\\nカラムの一列目の名称: {df.columns[0]}\")\n",
    "\n",
    "\n",
    "def check_cell_type(df):\n",
    "    \"\"\"Cell_type列の値が想定される形であるか確認する\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "\n",
    "    \"\"\"\n",
    "    wrong_index_ls = list(df.loc[~df['Cell_type'].str.match(r'[0-9]+_[0-9]+'), 'Cell_type'])\n",
    "    if len(wrong_index_ls) >= 1:\n",
    "        raise ValueError(f'Cell_type列の中身が〇_〇の形になっていない行があります。\\n該当する値: {wrong_index_ls}')\n",
    "\n",
    "\n",
    "def check_targets(df, targets):\n",
    "    \"\"\"目的変数の入力が正しいか確認する\n",
    "    \n",
    "    - targetsに含まれる目的変数が無い場合にエラーが発生するか\n",
    "    - targetsが全角、スペース有りで記入（スクリプト上）された際、エラーが発生するか\n",
    "    - targetsに何も記入しなかった際に目的変数が選択されていないのでエラーが表示されるか\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "        targets (list): 対象の目的変数のリスト\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(targets) == 0:\n",
    "        raise ValueError(\"targetsが空です。\")\n",
    "\n",
    "    target_col_ls = [col for col in df.columns if 'T_' in col]\n",
    "    not_targets_ls = [t_col for t_col in targets if t_col not in target_col_ls]\n",
    "    \n",
    "    if len(not_targets_ls) >= 1:\n",
    "        raise ValueError(f\"以下の目的変数が入力データに含まれていません。\\n含まれていない目的変数名: {not_targets_ls}\")\n",
    "\n",
    "\n",
    "def check_str(df, targets):\n",
    "    \"\"\"文字列の有無を確認する\n",
    "\n",
    "    - 説明変数に文字列がある場合、エラーが発生するか\n",
    "    - 目的変数（target）に文字列がある場合、エラーが発生するか\n",
    "    - 目的変数（target以外）に文字列があった場合、エラーが発生しないか\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "        targets (list): 対象の目的変数のリスト\n",
    "\n",
    "    \"\"\"\n",
    "    target_col_ls = [col for col in df.columns if 'T_' in col]\n",
    "    not_targets_ls = [t_col for t_col in target_col_ls if t_col not in targets]\n",
    "    str_columns = list(df.drop(['Cell_type']+not_targets_ls, axis=1).select_dtypes('object').columns)\n",
    "    if len(str_columns) != 0:\n",
    "        raise ValueError(f\"以下のカラムには文字列が含まれています。\\n文字列を含む変数名: {str_columns}\")\n",
    "\n",
    "\n",
    "def check_unique_one(df):\n",
    "    \"\"\"ユニーク数が1のものを確認する\n",
    "\n",
    "    説明変数にユニーク数1がある際に対象の説明変数を列挙するか\n",
    "    目的変数（target）にユニーク数1があった際、メッセージが出力されるか\n",
    "    目的変数（target以外）にユニーク数1があった際、メッセージが出力されないか\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "\n",
    "    Returns:\n",
    "        Bool: ユニーク数が1のものがあったか否か\n",
    "\n",
    "    \"\"\"\n",
    "    target_col_ls = [col for col in df.columns if 'T_' in col]\n",
    "    df_feature = df.drop(target_col_ls, axis=1)\n",
    "    df_check = df.loc[:, list(df_feature.columns) + targets]\n",
    "    not_unique_columns = list(df_check.columns[df_check.nunique() <= 1])\n",
    "    if len(not_unique_columns) != 0:\n",
    "        print(f\"以下のカラムはユニークが1つです。\\nユニークが1つの変数名: {not_unique_columns}\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_experiment_condtion(df, models, targets):\n",
    "    \"\"\"データを確認する\n",
    "\n",
    "    以下の項目に対して、入力データが適切に前処理が施されているか、settingが正しく行われているかを確認する\n",
    "      - modelsの入力が正しいか\n",
    "      - 欠損値の有無の確認\n",
    "      - データに'Cell_type'のカラムが含まれているか\n",
    "      - データに設定したtargetsリストに含まれる目的変数が全て含まれているか\n",
    "      - 文字データの有無\n",
    "      - 値をユニークな1種類のものしか持たないようなカラムの有無の確認\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "        models (list): 用いる機械学習モデルの名称のリスト\n",
    "        targets (list): 対象の目的変数のリスト\n",
    "          \n",
    "    \"\"\"\n",
    "    check_models(models)\n",
    "    check_null(df)\n",
    "    check_column_first(df)\n",
    "    check_cell_type(df)\n",
    "    check_targets(df, targets)\n",
    "    check_str(df, targets)\n",
    "    isin_flg = check_unique_one(df)\n",
    "    if isin_flg:\n",
    "        return None\n",
    "\n",
    "    return print('適切に前処理が施されています')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "適切に前処理が施されています\n"
     ]
    }
   ],
   "source": [
    "# データセットの確認をします。\n",
    "check_experiment_condtion(df, models, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_targets_columns_not_in_input(df, targets):\n",
    "    \"\"\"指定する目的変数以外の目的変数を削除する\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータ.\n",
    "        targets (list): 指定する目的変数のリスト.\n",
    "          \n",
    "    Returns:\n",
    "        pd.DataFrame: 指定する目的変数以外の目的変数を削除したデータ.\n",
    "\n",
    "    \"\"\"\n",
    "    remove_columns = []\n",
    "    for column in df.columns:\n",
    "        if (column[:2] == 'T_') and (column not in targets):\n",
    "            remove_columns.append(column)\n",
    "    \n",
    "    return df.drop(remove_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元のデータセットから今回の目的変数に含まれない目的変数のカラムを削除する処理を行います\n",
    "df = remove_targets_columns_not_in_input(df, targets)\n",
    "\n",
    "# 今回の予測モデルの構築で用いる特徴量の名称のリストを作成します\n",
    "features = list(df.drop(['Cell_type'] + targets, axis=1).columns)\n",
    "\n",
    "# 正規化前のデータフレームのmin, maxと連続値 or ダミー変数であるかを記録したcsvファイルを保存します。\n",
    "features_min_max_path = f'{output_dir}/features_min_max'\n",
    "if not os.path.exists(features_min_max_path):\n",
    "    os.mkdir(features_min_max_path)\n",
    "\n",
    "min_max = df.loc[:, features].agg([min, max]).T\n",
    "min_max['type'] = ['dummy' if 'D_' in col else 'real' for col in features]\n",
    "\n",
    "min_max.T.to_csv(join(features_min_max_path, 'min_max_type.csv'), encoding='shift_jis')"
   ]
  },
  {
   "source": [
    "def log_transform(data, targets):\n",
    "    \"\"\"対数変換を行う関数\n",
    "\n",
    "    Args:\n",
    "        data(pd.DataFrame): 対数変換を行うデータ.\n",
    "        targets(str or list of str): 対数変換を行うカラムの一覧.\n",
    "\n",
    "    Return:\n",
    "        log_data(pd.DataFrame): 対数変換されたデータ.\n",
    "\n",
    "    \"\"\"\n",
    "    log_data = data.copy()\n",
    "    if isinstance(targets, str):\n",
    "        targets = [targets]\n",
    "    else:\n",
    "        targets = list(targets)\n",
    "\n",
    "    for columns in targets:\n",
    "        log_data[columns] = np.log(log_data[columns])\n",
    "\n",
    "    return log_data\n",
    "\n",
    "\n",
    "def log_inverse_transform(data, targets):\n",
    "    \"\"\"逆対数変換を行う関数\n",
    "\n",
    "    Args:\n",
    "        data(pd.DataFrame): 逆対数変換を行うデータ.\n",
    "        targets(str or list of str): 逆対数変換を行うカラムの一覧.\n",
    "\n",
    "    Return:\n",
    "        log_inverse_data(pd.DataFrame): 逆対数変換されたデータ.\n",
    "\n",
    "     \"\"\"\n",
    "    log_inverse_data = data.copy()\n",
    "    if isinstance(targets, str):\n",
    "        targets = [targets]\n",
    "    else:\n",
    "        targets = list(targets)\n",
    "\n",
    "    for columns in targets:\n",
    "        log_inverse_data[columns] = np.exp(log_inverse_data[columns])\n",
    "\n",
    "    return log_inverse_data"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的変数に対して、対数変換を行います.\n",
    "if scale == 'Log':\n",
    "    df = log_transform(df, targets)"
   ]
  },
  {
   "source": [
    "# Part:4 データの標準化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_data(input_data, features, targets):\n",
    "    \"\"\"目的変数に対応したデータセットを作成する.\n",
    "\n",
    "    各目的変数ごとに値が欠損している行を削除したデータフレームを目的変数名に紐付けて保存する.\n",
    "\n",
    "    Args:\n",
    "        input_data (pd.DataFrame): 説明変数と目的変数を持つデータ.\n",
    "        features (list): 説明変数のリスト.\n",
    "        targets (list): 目的変数のリスト.\n",
    "      \n",
    "    Returns:\n",
    "      output_dict (dict): 各目的変数に対応したデータを持つ辞書.\n",
    "\n",
    "    \"\"\"\n",
    "    output_dict = {}\n",
    "    for target in targets:\n",
    "        output_dict[target] = (input_data.loc[:, ['Cell_type'] + features + [target]]\n",
    "                                         .dropna(how='any', axis=0)\n",
    "                                         .reset_index(drop='True'))\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的変数毎にデータセットを分割します。\n",
    "df_dic = create_target_data(df, features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_standard_scale(output_dir_path, data_dict, targets):\n",
    "    \"\"\"データに対し標準化を行う.\n",
    "\n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        data_dict (dict): 目的変数をキーとするデータの辞書.\n",
    "        targets (list): 目的変数のリスト.\n",
    "      \n",
    "    Returns:\n",
    "        output_dict (dict): 各目的変数に対応した標準化済みデータを持つ辞書.\n",
    "\n",
    "    \"\"\"\n",
    "    # 標準化オブジェクトと標準化後のデータフレームを保存するフォルダを作成\n",
    "    save_path_dict = {'sc_object': join(output_dir_path, 'sc_object'),\n",
    "                      'normalized_data': join(output_dir_path, 'normalized_data')}\n",
    "    for path in save_path_dict.values():\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    # 標準化オブジェクトの設定\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    output_dict = {}\n",
    "    for target in targets:\n",
    "        target_data = data_dict[target]\n",
    "\n",
    "        df_sc = sc.fit_transform(target_data.drop('Cell_type', axis=1))\n",
    "        df_sc = pd.concat([pd.DataFrame(target_data['Cell_type'], columns=['Cell_type']), \n",
    "                           pd.DataFrame(df_sc, columns=target_data.drop('Cell_type', axis=1).columns)],\n",
    "                          axis=1)\n",
    "        \n",
    "        output_dict[target] = df_sc\n",
    "\n",
    "        # オブジェクトを保存する.\n",
    "        df_sc.to_csv(join(save_path_dict['normalized_data'], f'{target}_normalized_data.csv'),\n",
    "                     encoding='shift_jis', index=False)\n",
    "        with open(join(save_path_dict['sc_object'], f'{target}_sc_object.pickle'), 'wb') as f:\n",
    "            pickle.dump(sc, f)\n",
    "\n",
    "        (pd.DataFrame({'mean': sc.mean_, 'std': sc.scale_},\n",
    "                      index=df_sc.drop(columns='Cell_type').columns)\n",
    "           .to_csv(join(save_path_dict['sc_object'], f'{target}_sc_object.csv'), encoding='shift_jis'))\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの標準化を行います。\n",
    "df_dic = transform_standard_scale(output_dir, df_dic, targets)"
   ]
  },
  {
   "source": [
    "# Part:5 学習データと評価データの分割"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_by_celltype(df, test_size=TEST_SIZE, random_state=RANDOM_STATE):\n",
    "    \"\"\"データセットをセルタイプに応じて均等に学習データと評価データに分割する.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 分割を行うデータ.\n",
    "        test_size (float): 分割時の評価データの比率.\n",
    "        random_state (int): ランダムシード値.\n",
    "\n",
    "    Returns:\n",
    "        df_train (pd.DataFrame): 学習データ.\n",
    "        df_test (pd.DataFrame): 評価データ.\n",
    "\n",
    "    Notes:\n",
    "        Cell_typeの行の表示名がセルタイプ_XXの形になっていること. ex) \"1_11\"\n",
    "\n",
    "    \"\"\"\n",
    "    # 結果を格納する空のデータフレームを作る.\n",
    "    df_train, df_test = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # 1タイプごとにデータを分割する.\n",
    "    for _, one_type_data in df.groupby(df['Cell_type'].str.split('_', expand=True)[0]):\n",
    "        trains, tests = train_test_split(one_type_data, test_size=test_size,\n",
    "                                         random_state=random_state)\n",
    "\n",
    "        df_train = pd.concat([df_train, trains])\n",
    "        df_test = pd.concat([df_test, tests])\n",
    "\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データセットを学習データと評価データへ分割します。\n",
    "for target in targets:\n",
    "    df_target_train_sc, df_target_test_sc = data_split_by_celltype(df_dic[target],\n",
    "                                                                   test_size=TEST_SIZE,\n",
    "                                                                   random_state=RANDOM_STATE)\n",
    "\n",
    "    save_dir = f'{output_dir}/split_data/{target}'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    df_target_train_sc.to_csv(join(save_dir, 'train.csv'), encoding=\"shift_jis\", index=False)\n",
    "    df_target_test_sc.to_csv(join(save_dir, 'test.csv'), encoding=\"shift_jis\", index=False)\n",
    "\n",
    "    df_dic[target] = (df_target_train_sc, df_target_test_sc)"
   ]
  },
  {
   "source": [
    "# Part:6 予測モデルの学習"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"モデルを取得する\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): モデルの名前.\n",
    "    \n",
    "    Returns:\n",
    "        class: モデル.\n",
    "\n",
    "    \"\"\"\n",
    "    model_dict = {\"LinearRegression\": LinearRegression(),\n",
    "                  \"Ridge\": Ridge(),\n",
    "                  \"Lasso\": Lasso(),\n",
    "                  \"ElasticNet\": ElasticNet(),\n",
    "                  \"GBDT\": GradientBoostingRegressor(),\n",
    "                  \"AdaBoost\": AdaBoostRegressor(),\n",
    "                  \"RandomForest\": RandomForestRegressor(),\n",
    "                  \"SVR\": SVR(),\n",
    "                  \"MLP\": MLPRegressor()}\n",
    "\n",
    "    model = model_dict.get(model_name)\n",
    "\n",
    "    if model is None:\n",
    "        raise ValueError(f'model_nameが違います: {model_name}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def models_train(output_dir_path, df_dic, features, targets, models, folder_name, model_param_dict):\n",
    "    \"\"\"目的変数のリストと用いる機械学習モデルの集合を受け取り学習したモデルを出力する関数.\n",
    "\n",
    "    各データに対してモデルを学習し、pickleファイルで保存する.\n",
    "\n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        df_dic (dict): key: 目的変数名、values: (学習データ, 評価データ)とする辞書.\n",
    "        features (list): 説明変数のリスト.\n",
    "        targets (list): 目的変数のリスト.\n",
    "        models (list): 用いる機械学習モデルの名称のリスト. 現状では以下の9つの候補から選択.\n",
    " 　                   {\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\",\n",
    "                       \"GBDT\", \"AdaBoost\", \"RandomForest\", \"SVR\", \"MLP\"}\n",
    "        folder_name (str): 今回の実験の各種記録を行っているフォルダの名前\n",
    "        model_param_dict (dict): key: モデル名, values: パラメータの辞書とする各モデルのパラメータの辞書. \n",
    "\n",
    "    Notes:\n",
    "        線形モデルをfitする際、LinAlgErrorが発生する場合があります。\n",
    "        LinAlgErrorが発生した場合、再度fitを行います。\n",
    "        10回繰り返してもエラーが発生する場合は処理が止まります。\n",
    "\n",
    "        LinAlgError: 最小二乗法を行った時に、SDVの値が収束しない場合にLinAlgErrorが発生します。\n",
    "        参考URL: http://oppython.hatenablog.com/entry/2014/01/21/003245\n",
    "\n",
    "    \"\"\"\n",
    "    for target in targets:\n",
    "        print(f'学習を開始します。目的変数: {target}')\n",
    "        path_train_target = join(output_dir_path, f'trained_model/{target}')\n",
    "        if not os.path.exists(path_train_target):\n",
    "            os.makedirs(path_train_target)\n",
    "\n",
    "        df_train, _ = df_dic[target]\n",
    "        for model_name in models:\n",
    "            model = get_model(model_name)\n",
    "            model.set_params(**model_param_dict[model_name])\n",
    "            # numpyのライブラリー起因でSVDが求まらない時があるため、10回までmodel.fit()を繰り返します。\n",
    "            # 詳細はdocstringのNotesと参考URLをご参照ください。\n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    model.fit(df_train[features], df_train[target])\n",
    "                    break\n",
    "                except LinAlgError:\n",
    "                    if i == 9:\n",
    "                        raise RuntimeError(f'学習に失敗しました. モデル名: {model_name}')\n",
    "                    else:\n",
    "                        print(f'\"{model_name}\" の学習時にLinAlgErrorが発生しました。再度学習します。')\n",
    "                        continue\n",
    "\n",
    "            filename = f'{folder_name}_{model_name}_{target}.pickle'\n",
    "            with open(join(path_train_target, filename), 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "    print('すべての目的変数に対して学習が完了しました。')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "学習を開始します。目的変数: T_2\n",
      "学習を開始します。目的変数: T_12\n",
      "学習を開始します。目的変数: T_1635\n",
      "すべての目的変数に対して学習が完了しました。\n"
     ]
    }
   ],
   "source": [
    "# 各モデル・データセット毎に学習を行います。\n",
    "models_train(output_dir, df_dic, features, targets, models, folder_name, model_param_dict)"
   ]
  },
  {
   "source": [
    "# Part:7 分析条件の保存"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験条件をjson型式で保存します。\n",
    "json_path = f'{output_dir}/experiment_info_json'\n",
    "if not os.path.exists(json_path):\n",
    "    os.makedirs(json_path)\n",
    "\n",
    "for model in models:\n",
    "    for target in targets:\n",
    "        experiment_info = {'target': target, 'features': features, 'scale': scale,\n",
    "                           'test_size': TEST_SIZE, 'model': model, 'random_state': RANDOM_STATE,\n",
    "                           'folder_name': folder_name, 'param': model_param_dict[model],\n",
    "                           'trained_model_path': f'{output_dir}/trained_model/{target}/{folder_name}_{model}_{target}.pickle',\n",
    "                           'sc_object_path': f'{output_dir}/sc_object/{target}_sc_object.pickle',\n",
    "                           'summary_statistics_path': f'{output_dir}/features_min_max/min_max_type.csv',\n",
    "                           'input_data_path': f'{input_file_dir_path}/{input_file_name}'}\n",
    "\n",
    "        with open(join(json_path, f'{model}_{target}_experiment_info.json'), 'w') as f:\n",
    "            json.dump(experiment_info, f, indent=2)"
   ]
  },
  {
   "source": [
    "# Part:8 評価データの予測"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_standard_scale(output_dir_path, data, target, scale):\n",
    "    \"\"\"標準化を戻す関数\n",
    "    \n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        data (pd.DataFrame): 標準化を戻すデータ.\n",
    "        target (str): 目的変数名.\n",
    "        scale (str): 目的変数のスケール. 'Linear' or `Log`.\n",
    "    \n",
    "    Returns:\n",
    "        output_data (pd.DataFrame): 標準化を戻したデータ.\n",
    "\n",
    "    \"\"\"\n",
    "    df_sc = pd.read_csv(f'{output_dir_path}/sc_object/{target}_sc_object.csv', index_col=0)\n",
    "    output_data = (data * df_sc.loc[target, 'std']) + df_sc.loc[target, 'mean']\n",
    "    \n",
    "    if scale == 'Log':\n",
    "        col_name = output_data.columns\n",
    "        output_data.columns = [target]\n",
    "        output_data = log_inverse_transform(output_data, target)\n",
    "        output_data.columns = col_name\n",
    "\n",
    "    return output_data\n",
    "\n",
    "\n",
    "def models_predicts(output_dir_path, df_dic, features, targets, folder_name, scale='Linear'):\n",
    "    \"\"\"学習後のモデルを用いて評価データの予測値を求めcsv形式で保存する.\n",
    "\n",
    "    各種目的変数に対して各種モデルの予測値を「models/folder_name/predicts/'スケール'/'目的変数名'」 の配下に\n",
    "    「folder_name_'モデル名'_'目的変数名'.csv」という名称で出力する\n",
    "\n",
    "    各種目的変数の値を「models/folder_name/observes/'スケール'/'目的変数名'」 の配下に\n",
    "    「folder_name_'目的変数名'.csv」という名称で出力する\n",
    "    \n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        df_dic (dict): key: 目的変数名、values: (学習データ, 評価データ)とする辞書.\n",
    "        features (list): 説明変数のリスト.\n",
    "        targets (list): 目的変数のリスト.\n",
    "        folder_name (str): 今回の実験の各種記録を行っているフォルダの名前.\n",
    "        scale (str): 目的変数のスケール. 'Linear' or `Log`.\n",
    "\n",
    "    \"\"\"\n",
    "    for target in targets:\n",
    "        _, df_test = df_dic[target]\n",
    "        \n",
    "        # 予測結果を格納するフォルダ\n",
    "        path_predicts_dict = {'standard_scale': f'{output_dir_path}/predicts/standard_scale/{target}',\n",
    "                              'raw_scale': f'{output_dir_path}/predicts/raw_scale/{target}'}\n",
    "        \n",
    "        # 目的変数の値を格納するフォルダ\n",
    "        path_observes_dict = {'standard_scale': f'{output_dir_path}/observes/standard_scale/{target}',\n",
    "                              'raw_scale': f'{output_dir_path}/observes/raw_scale/{target}'}\n",
    "\n",
    "        for path_dict in [path_predicts_dict, path_observes_dict]:\n",
    "            for path in path_dict.values():\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "\n",
    "        # 目的変数の値を保存\n",
    "        df_obs = df_test.set_index('Cell_type')[[target]]\n",
    "        df_obs.to_csv(join(path_observes_dict['standard_scale'], f'{folder_name}_{target}.csv'))\n",
    "        df_obs_raw = inverse_standard_scale(output_dir_path, df_obs, target, scale)\n",
    "        df_obs_raw.to_csv(join(path_observes_dict['raw_scale'], f'{folder_name}_{target}.csv'))\n",
    "\n",
    "        # 学習済みモデルのパスを取得\n",
    "        trained_files = os.listdir(join(output_dir_path, f'trained_model/{target}'))\n",
    "        for trained_file in trained_files:\n",
    "            with open(f'{output_dir_path}/trained_model/{target}/{trained_file}', 'rb')as f:\n",
    "                model = pickle.load(f)\n",
    "\n",
    "            df_pred = pd.DataFrame(model.predict(df_test[features]),\n",
    "                                   columns=['predicts'], index=df_test['Cell_type'])\n",
    "            \n",
    "            file_name = f'{folder_name}_{trained_file[13:-4]}.csv'\n",
    "            df_pred.to_csv(join(path_predicts_dict['standard_scale'], file_name))\n",
    "\n",
    "            df_pred_raw = inverse_standard_scale(output_dir_path, df_pred, target, scale)\n",
    "            df_pred_raw.to_csv(join(path_predicts_dict['raw_scale'], file_name))\n",
    "\n",
    "    print('予測が完了しました')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "予測が完了しました\n"
     ]
    }
   ],
   "source": [
    "# 各モデル・データセットごとに予測値を算出します。\n",
    "models_predicts(output_dir, df_dic, features, targets, folder_name, scale=scale)"
   ]
  },
  {
   "source": [
    "# Part:9 評価指標の算出"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(test, pred):\n",
    "    \"\"\"MAPEスコアを計算する.\n",
    "    \n",
    "    Args:\n",
    "        test (np.array): 実測値.\n",
    "        pred (np.array): 予測値.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: MAPEの値.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((pred - test) / test)) * 100\n",
    "\n",
    "\n",
    "def smape(test, pred):\n",
    "    \"\"\"S-MAPEスコアを計算する.\n",
    "    \n",
    "    Args:\n",
    "        test (np.array): 実測値.\n",
    "        pred (np.array): 予測値.\n",
    "\n",
    "    Returns:\n",
    "        np.array: S-MAPEの値.\n",
    "    \n",
    "    \"\"\"\n",
    "    return 100 / len(test) * np.sum(2 * np.abs(pred - test) / (np.abs(pred) + np.abs(test)))\n",
    "\n",
    "\n",
    "def max_error_rate(test, pred):\n",
    "    \"\"\"真の値と予測値の誤差の差分割合の最大値を計算する.\n",
    "    \n",
    "   Args:\n",
    "        test (np.array): 実測値.\n",
    "        pred (np.array): 予測値.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: max_error_rate(MER)の値.\n",
    "\n",
    "    \"\"\"\n",
    "    dif = np.abs(np.subtract(test, pred))\n",
    "    mer = np.abs(max(dif) / test[dif.argmax()])\n",
    "\n",
    "    return mer * 100\n",
    "\n",
    "\n",
    "def calculate_scores(test, pred, p, model_name):\n",
    "    \"\"\"7つのスコア(R2, MAE, MSE, RMSE, MAPE, S-MAPE, MER)を計算してデータフレーム形式で結果を出力する.\n",
    "\n",
    "    Args:\n",
    "        test (np.array): 実測値.\n",
    "        pred (np.array): 予測値.\n",
    "        p (int): 説明変数の個数.\n",
    "        model_name (str): 予測に用いたモデルの名前\n",
    "    \n",
    "    Returns:\n",
    "        score_comparisons (pd.DataFrame): 各スコアの結果をまとめたデータフレーム.\n",
    "\n",
    "    \"\"\"\n",
    "    test = np.array(test)\n",
    "    pred = np.array(pred)\n",
    "    scores = pd.DataFrame({'r2_score': r2_score(test, pred),\n",
    "                           'mae_score': mean_absolute_error(test, pred),\n",
    "                           'mse_score': mean_squared_error(test, pred),\n",
    "                           'rmse_score': np.sqrt(mean_squared_error(test, pred)),\n",
    "                           'mape_score': mape(test, pred),\n",
    "                           'smape_score': smape(test, pred),\n",
    "                           'max_error_rate': max_error_rate(test, pred)},\n",
    "                           index=[model_name])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_scores(output_dir_path, targets, models, folder_name):\n",
    "    \"\"\"評価指標の一覧を表示する\n",
    "\n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        targets (list): 目的変数のリスト.\n",
    "        models (list): 用いる機械学習モデルの名称のリスト. 現状では以下の9つの候補から選択.\n",
    " 　                   {\"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\",\n",
    "                       \"GBDT\", \"AdaBoost\", \"RandomForest\", \"SVR\", \"MLP\"}\n",
    "        folder_name (str): 今回の実験の各種記録を行っているフォルダの名前.\n",
    "        \n",
    "    \"\"\"\n",
    "    for target in targets:\n",
    "        # 評価指標を格納するフォルダ\n",
    "        path_score_dict = {'standard_scale': f'{output_dir_path}/score/standard_scale/{target}',\n",
    "                           'raw_scale': f'{output_dir_path}/score/raw_scale/{target}'}\n",
    "        for path in path_score_dict.values():\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        # 予測結果を格納しているフォルダ\n",
    "        path_predicts_dict = {'standard_scale': f'{output_dir_path}/predicts/standard_scale/{target}',\n",
    "                              'raw_scale': f'{output_dir_path}/predicts/raw_scale/{target}'}\n",
    "\n",
    "        # 目的変数の値を格納しているフォルダ\n",
    "        path_observes_dict = {'standard_scale': f'{output_dir_path}/observes/standard_scale/{target}',\n",
    "                              'raw_scale': f'{output_dir_path}/observes/raw_scale/{target}'}\n",
    "\n",
    "        for scale_name in ['standard_scale', 'raw_scale']:\n",
    "            print(f'目的変数名: {target}, 目的変数のスケール: {scale_name}')\n",
    "            \n",
    "            score_comparison = pd.DataFrame()\n",
    "            test = pd.read_csv(join(path_observes_dict[scale_name], f'{folder_name}_{target}.csv'),\n",
    "                               index_col=0)[target]\n",
    "\n",
    "            for predict_file in os.listdir(path_predicts_dict[scale_name]):\n",
    "                pred = pd.read_csv(join(path_predicts_dict[scale_name], predict_file), index_col=0)['predicts']\n",
    "                \n",
    "                # 予測値が無限大になるサンプルを削除する.\n",
    "                test_pred = pd.DataFrame({'test': test, 'pred': pred})\n",
    "                drop_cell_types = list(test_pred.loc[test_pred['pred'] == np.inf, :].index)\n",
    "                if len(drop_cell_types) >= 1:\n",
    "                    print(f' > {predict_file[13:-7]}において、予測値が無限大になっているサンプルがあります。\\n',\n",
    "                          f'> 以下のサンプルを削除して評価指標は計算しています。\\n',\n",
    "                          f'> 削除するサンプル: {drop_cell_types}')\n",
    "                test_pred = test_pred.drop(index=drop_cell_types)\n",
    "\n",
    "                scores = calculate_scores(test_pred['test'], test_pred['pred'],\n",
    "                                          len(features), predict_file[13:-7])\n",
    "                score_comparison = pd.concat([score_comparison, scores])\n",
    "\n",
    "            algorithm_order_target = [f'{algorithm}_{target}' for algorithm in models]\n",
    "            display_score_comparison = score_comparison.reindex(index=algorithm_order_target)\n",
    "            display_score_comparison.to_csv(join(path_score_dict[scale_name], f'{folder_name}_{target}_score.csv'),\n",
    "                                            encoding='shift_jis')\n",
    "            \n",
    "            display(display_score_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "目的変数名: T_2, 目的変数のスケール: standard_scale\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                          r2_score     mae_score     mse_score    rmse_score  \\\nLinearRegression_T_2 -3.640411e+23  4.721424e+10  3.310084e+23  5.753333e+11   \nRidge_T_2             9.552703e-01  4.438746e-02  4.067100e-02  2.016705e-01   \nLasso_T_2             8.942111e-01  6.904848e-02  9.618977e-02  3.101448e-01   \nElasticNet_T_2        9.452480e-01  4.811428e-02  4.978391e-02  2.231231e-01   \nGBDT_T_2              9.962903e-01  9.958113e-03  3.373078e-03  5.807821e-02   \nAdaBoost_T_2          9.874856e-01  2.514565e-02  1.137883e-02  1.066716e-01   \nRandomForest_T_2      9.914209e-01  1.358927e-02  7.800623e-03  8.832113e-02   \nSVR_T_2               9.490437e-01  9.344939e-02  4.633255e-02  2.152500e-01   \nMLP_T_2               8.632096e-01  7.675101e-02  1.243782e-01  3.526729e-01   \n\n                        mape_score  smape_score  max_error_rate  \nLinearRegression_T_2  2.835951e+13    14.654730    6.110993e+15  \nRidge_T_2             3.045365e+01     9.967452    5.327655e+01  \nLasso_T_2             2.470654e+01    22.940846    3.324282e+01  \nElasticNet_T_2        2.236017e+01    17.122999    2.425379e+01  \nGBDT_T_2              5.167142e+00     3.374557    6.634753e+01  \nAdaBoost_T_2          1.545278e+01    12.129208    1.118770e+02  \nRandomForest_T_2      3.863210e+00     3.015696    8.448543e+00  \nSVR_T_2               5.595761e+01    58.221158    1.242273e+03  \nMLP_T_2               5.141756e+01    17.025640    1.914060e+03  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r2_score</th>\n      <th>mae_score</th>\n      <th>mse_score</th>\n      <th>rmse_score</th>\n      <th>mape_score</th>\n      <th>smape_score</th>\n      <th>max_error_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LinearRegression_T_2</th>\n      <td>-3.640411e+23</td>\n      <td>4.721424e+10</td>\n      <td>3.310084e+23</td>\n      <td>5.753333e+11</td>\n      <td>2.835951e+13</td>\n      <td>14.654730</td>\n      <td>6.110993e+15</td>\n    </tr>\n    <tr>\n      <th>Ridge_T_2</th>\n      <td>9.552703e-01</td>\n      <td>4.438746e-02</td>\n      <td>4.067100e-02</td>\n      <td>2.016705e-01</td>\n      <td>3.045365e+01</td>\n      <td>9.967452</td>\n      <td>5.327655e+01</td>\n    </tr>\n    <tr>\n      <th>Lasso_T_2</th>\n      <td>8.942111e-01</td>\n      <td>6.904848e-02</td>\n      <td>9.618977e-02</td>\n      <td>3.101448e-01</td>\n      <td>2.470654e+01</td>\n      <td>22.940846</td>\n      <td>3.324282e+01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet_T_2</th>\n      <td>9.452480e-01</td>\n      <td>4.811428e-02</td>\n      <td>4.978391e-02</td>\n      <td>2.231231e-01</td>\n      <td>2.236017e+01</td>\n      <td>17.122999</td>\n      <td>2.425379e+01</td>\n    </tr>\n    <tr>\n      <th>GBDT_T_2</th>\n      <td>9.962903e-01</td>\n      <td>9.958113e-03</td>\n      <td>3.373078e-03</td>\n      <td>5.807821e-02</td>\n      <td>5.167142e+00</td>\n      <td>3.374557</td>\n      <td>6.634753e+01</td>\n    </tr>\n    <tr>\n      <th>AdaBoost_T_2</th>\n      <td>9.874856e-01</td>\n      <td>2.514565e-02</td>\n      <td>1.137883e-02</td>\n      <td>1.066716e-01</td>\n      <td>1.545278e+01</td>\n      <td>12.129208</td>\n      <td>1.118770e+02</td>\n    </tr>\n    <tr>\n      <th>RandomForest_T_2</th>\n      <td>9.914209e-01</td>\n      <td>1.358927e-02</td>\n      <td>7.800623e-03</td>\n      <td>8.832113e-02</td>\n      <td>3.863210e+00</td>\n      <td>3.015696</td>\n      <td>8.448543e+00</td>\n    </tr>\n    <tr>\n      <th>SVR_T_2</th>\n      <td>9.490437e-01</td>\n      <td>9.344939e-02</td>\n      <td>4.633255e-02</td>\n      <td>2.152500e-01</td>\n      <td>5.595761e+01</td>\n      <td>58.221158</td>\n      <td>1.242273e+03</td>\n    </tr>\n    <tr>\n      <th>MLP_T_2</th>\n      <td>8.632096e-01</td>\n      <td>7.675101e-02</td>\n      <td>1.243782e-01</td>\n      <td>3.526729e-01</td>\n      <td>5.141756e+01</td>\n      <td>17.025640</td>\n      <td>1.914060e+03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "目的変数名: T_2, 目的変数のスケール: raw_scale\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                          r2_score     mae_score     mse_score    rmse_score  \\\nLinearRegression_T_2 -3.640411e+23  6.548965e+11  6.368518e+25  7.980300e+12   \nRidge_T_2             9.552703e-01  6.156870e-01  7.824997e+00  2.797320e+00   \nLasso_T_2             8.942111e-01  9.577536e-01  1.850667e+01  4.301938e+00   \nElasticNet_T_2        9.452480e-01  6.673807e-01  9.578299e+00  3.094883e+00   \nGBDT_T_2              9.962903e-01  1.381264e-01  6.489717e-01  8.055878e-01   \nAdaBoost_T_2          9.874856e-01  3.487888e-01  2.189258e+00  1.479614e+00   \nRandomForest_T_2      9.914209e-01  1.884932e-01  1.500820e+00  1.225080e+00   \nSVR_T_2               9.490437e-01  1.296212e+00  8.914266e+00  2.985677e+00   \nMLP_T_2               8.632096e-01  1.064593e+00  2.393005e+01  4.891835e+00   \n\n                        mape_score  smape_score  max_error_rate  \nLinearRegression_T_2  2.222752e+13   130.581102    2.970097e+15  \nRidge_T_2             5.453661e+02   136.434117    5.102149e+01  \nLasso_T_2             2.546328e+03   152.720429    3.284415e+01  \nElasticNet_T_2        1.741354e+03   142.258536    2.396292e+01  \nGBDT_T_2              7.997889e+01    53.656155    5.739603e+01  \nAdaBoost_T_2          6.963834e+02   130.454047    9.678270e+01  \nRandomForest_T_2      5.146665e+00     5.114614    8.347222e+00  \nSVR_T_2               4.814282e+03   169.727338    5.044673e+04  \nMLP_T_2               8.945573e+02   153.610689    7.772695e+04  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r2_score</th>\n      <th>mae_score</th>\n      <th>mse_score</th>\n      <th>rmse_score</th>\n      <th>mape_score</th>\n      <th>smape_score</th>\n      <th>max_error_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LinearRegression_T_2</th>\n      <td>-3.640411e+23</td>\n      <td>6.548965e+11</td>\n      <td>6.368518e+25</td>\n      <td>7.980300e+12</td>\n      <td>2.222752e+13</td>\n      <td>130.581102</td>\n      <td>2.970097e+15</td>\n    </tr>\n    <tr>\n      <th>Ridge_T_2</th>\n      <td>9.552703e-01</td>\n      <td>6.156870e-01</td>\n      <td>7.824997e+00</td>\n      <td>2.797320e+00</td>\n      <td>5.453661e+02</td>\n      <td>136.434117</td>\n      <td>5.102149e+01</td>\n    </tr>\n    <tr>\n      <th>Lasso_T_2</th>\n      <td>8.942111e-01</td>\n      <td>9.577536e-01</td>\n      <td>1.850667e+01</td>\n      <td>4.301938e+00</td>\n      <td>2.546328e+03</td>\n      <td>152.720429</td>\n      <td>3.284415e+01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet_T_2</th>\n      <td>9.452480e-01</td>\n      <td>6.673807e-01</td>\n      <td>9.578299e+00</td>\n      <td>3.094883e+00</td>\n      <td>1.741354e+03</td>\n      <td>142.258536</td>\n      <td>2.396292e+01</td>\n    </tr>\n    <tr>\n      <th>GBDT_T_2</th>\n      <td>9.962903e-01</td>\n      <td>1.381264e-01</td>\n      <td>6.489717e-01</td>\n      <td>8.055878e-01</td>\n      <td>7.997889e+01</td>\n      <td>53.656155</td>\n      <td>5.739603e+01</td>\n    </tr>\n    <tr>\n      <th>AdaBoost_T_2</th>\n      <td>9.874856e-01</td>\n      <td>3.487888e-01</td>\n      <td>2.189258e+00</td>\n      <td>1.479614e+00</td>\n      <td>6.963834e+02</td>\n      <td>130.454047</td>\n      <td>9.678270e+01</td>\n    </tr>\n    <tr>\n      <th>RandomForest_T_2</th>\n      <td>9.914209e-01</td>\n      <td>1.884932e-01</td>\n      <td>1.500820e+00</td>\n      <td>1.225080e+00</td>\n      <td>5.146665e+00</td>\n      <td>5.114614</td>\n      <td>8.347222e+00</td>\n    </tr>\n    <tr>\n      <th>SVR_T_2</th>\n      <td>9.490437e-01</td>\n      <td>1.296212e+00</td>\n      <td>8.914266e+00</td>\n      <td>2.985677e+00</td>\n      <td>4.814282e+03</td>\n      <td>169.727338</td>\n      <td>5.044673e+04</td>\n    </tr>\n    <tr>\n      <th>MLP_T_2</th>\n      <td>8.632096e-01</td>\n      <td>1.064593e+00</td>\n      <td>2.393005e+01</td>\n      <td>4.891835e+00</td>\n      <td>8.945573e+02</td>\n      <td>153.610689</td>\n      <td>7.772695e+04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "目的変数名: T_12, 目的変数のスケール: standard_scale\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                           r2_score     mae_score     mse_score    rmse_score  \\\nLinearRegression_T_12 -9.320622e+23  1.218107e+11  8.708612e+23  9.331995e+11   \nRidge_T_12             9.527980e-01  1.207612e-01  4.410264e-02  2.100063e-01   \nLasso_T_12             8.659496e-01  2.917061e-01  1.252484e-01  3.539045e-01   \nElasticNet_T_12        8.998225e-01  2.387627e-01  9.359963e-02  3.059406e-01   \nGBDT_T_12              9.563871e-01  1.112611e-01  4.074923e-02  2.018644e-01   \nAdaBoost_T_12          8.824441e-01  2.642111e-01  1.098370e-01  3.314167e-01   \nRandomForest_T_12      9.478674e-01  1.164874e-01  4.870948e-02  2.207023e-01   \nSVR_T_12               9.432010e-01  1.259356e-01  5.306950e-02  2.303682e-01   \nMLP_T_12               9.079251e-01  1.374234e-01  8.602911e-02  2.933072e-01   \n\n                         mape_score  smape_score  max_error_rate  \nLinearRegression_T_12  1.811411e+13    32.052776    1.628493e+15  \nRidge_T_12             2.361262e+01    22.487804    2.211408e+02  \nLasso_T_12             4.654492e+01    46.282850    6.719155e+01  \nElasticNet_T_12        4.003823e+01    37.477738    5.627435e+01  \nGBDT_T_12              2.295320e+01    22.898721    2.198371e+02  \nAdaBoost_T_12          5.310786e+01    65.639429    1.994091e+02  \nRandomForest_T_12      2.438195e+01    24.231021    2.211040e+02  \nSVR_T_12               2.529672e+01    22.438494    2.139155e+02  \nMLP_T_12               2.732774e+01    24.849035    3.822742e+02  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r2_score</th>\n      <th>mae_score</th>\n      <th>mse_score</th>\n      <th>rmse_score</th>\n      <th>mape_score</th>\n      <th>smape_score</th>\n      <th>max_error_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LinearRegression_T_12</th>\n      <td>-9.320622e+23</td>\n      <td>1.218107e+11</td>\n      <td>8.708612e+23</td>\n      <td>9.331995e+11</td>\n      <td>1.811411e+13</td>\n      <td>32.052776</td>\n      <td>1.628493e+15</td>\n    </tr>\n    <tr>\n      <th>Ridge_T_12</th>\n      <td>9.527980e-01</td>\n      <td>1.207612e-01</td>\n      <td>4.410264e-02</td>\n      <td>2.100063e-01</td>\n      <td>2.361262e+01</td>\n      <td>22.487804</td>\n      <td>2.211408e+02</td>\n    </tr>\n    <tr>\n      <th>Lasso_T_12</th>\n      <td>8.659496e-01</td>\n      <td>2.917061e-01</td>\n      <td>1.252484e-01</td>\n      <td>3.539045e-01</td>\n      <td>4.654492e+01</td>\n      <td>46.282850</td>\n      <td>6.719155e+01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet_T_12</th>\n      <td>8.998225e-01</td>\n      <td>2.387627e-01</td>\n      <td>9.359963e-02</td>\n      <td>3.059406e-01</td>\n      <td>4.003823e+01</td>\n      <td>37.477738</td>\n      <td>5.627435e+01</td>\n    </tr>\n    <tr>\n      <th>GBDT_T_12</th>\n      <td>9.563871e-01</td>\n      <td>1.112611e-01</td>\n      <td>4.074923e-02</td>\n      <td>2.018644e-01</td>\n      <td>2.295320e+01</td>\n      <td>22.898721</td>\n      <td>2.198371e+02</td>\n    </tr>\n    <tr>\n      <th>AdaBoost_T_12</th>\n      <td>8.824441e-01</td>\n      <td>2.642111e-01</td>\n      <td>1.098370e-01</td>\n      <td>3.314167e-01</td>\n      <td>5.310786e+01</td>\n      <td>65.639429</td>\n      <td>1.994091e+02</td>\n    </tr>\n    <tr>\n      <th>RandomForest_T_12</th>\n      <td>9.478674e-01</td>\n      <td>1.164874e-01</td>\n      <td>4.870948e-02</td>\n      <td>2.207023e-01</td>\n      <td>2.438195e+01</td>\n      <td>24.231021</td>\n      <td>2.211040e+02</td>\n    </tr>\n    <tr>\n      <th>SVR_T_12</th>\n      <td>9.432010e-01</td>\n      <td>1.259356e-01</td>\n      <td>5.306950e-02</td>\n      <td>2.303682e-01</td>\n      <td>2.529672e+01</td>\n      <td>22.438494</td>\n      <td>2.139155e+02</td>\n    </tr>\n    <tr>\n      <th>MLP_T_12</th>\n      <td>9.079251e-01</td>\n      <td>1.374234e-01</td>\n      <td>8.602911e-02</td>\n      <td>2.933072e-01</td>\n      <td>2.732774e+01</td>\n      <td>24.849035</td>\n      <td>3.822742e+02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "目的変数名: T_12, 目的変数のスケール: raw_scale\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                           r2_score     mae_score     mse_score    rmse_score  \\\nLinearRegression_T_12 -9.320622e+23  1.791215e+14  1.883102e+30  1.372262e+15   \nRidge_T_12             9.527980e-01  1.775783e+02  9.536508e+04  3.088124e+02   \nLasso_T_12             8.659496e-01  4.289512e+02  2.708301e+05  5.204134e+02   \nElasticNet_T_12        8.998225e-01  3.510984e+02  2.023946e+05  4.498829e+02   \nGBDT_T_12              9.563871e-01  1.636085e+02  8.811387e+04  2.968398e+02   \nAdaBoost_T_12          8.824441e-01  3.885201e+02  2.375054e+05  4.873453e+02   \nRandomForest_T_12      9.478674e-01  1.712937e+02  1.053267e+05  3.245407e+02   \nSVR_T_12               9.432010e-01  1.851871e+02  1.147545e+05  3.387544e+02   \nMLP_T_12               9.079251e-01  2.020799e+02  1.860246e+05  4.313056e+02   \n\n                         mape_score  smape_score  max_error_rate  \nLinearRegression_T_12  6.554119e+13    55.143750    1.247587e+16  \nRidge_T_12             5.626656e+01    41.685802    8.034914e+01  \nLasso_T_12             5.384263e+02    72.651593    4.691859e+01  \nElasticNet_T_12        4.587071e+02    63.968125    3.929532e+01  \nGBDT_T_12              5.140306e+01    35.449737    7.987547e+01  \nAdaBoost_T_12          3.206372e+02    72.501307    7.245316e+01  \nRandomForest_T_12      4.325115e+01    28.243059    8.033578e+01  \nSVR_T_12               1.709560e+02    44.859362    1.638805e+03  \nMLP_T_12               8.593241e+01    42.846015    2.928600e+03  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r2_score</th>\n      <th>mae_score</th>\n      <th>mse_score</th>\n      <th>rmse_score</th>\n      <th>mape_score</th>\n      <th>smape_score</th>\n      <th>max_error_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LinearRegression_T_12</th>\n      <td>-9.320622e+23</td>\n      <td>1.791215e+14</td>\n      <td>1.883102e+30</td>\n      <td>1.372262e+15</td>\n      <td>6.554119e+13</td>\n      <td>55.143750</td>\n      <td>1.247587e+16</td>\n    </tr>\n    <tr>\n      <th>Ridge_T_12</th>\n      <td>9.527980e-01</td>\n      <td>1.775783e+02</td>\n      <td>9.536508e+04</td>\n      <td>3.088124e+02</td>\n      <td>5.626656e+01</td>\n      <td>41.685802</td>\n      <td>8.034914e+01</td>\n    </tr>\n    <tr>\n      <th>Lasso_T_12</th>\n      <td>8.659496e-01</td>\n      <td>4.289512e+02</td>\n      <td>2.708301e+05</td>\n      <td>5.204134e+02</td>\n      <td>5.384263e+02</td>\n      <td>72.651593</td>\n      <td>4.691859e+01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet_T_12</th>\n      <td>8.998225e-01</td>\n      <td>3.510984e+02</td>\n      <td>2.023946e+05</td>\n      <td>4.498829e+02</td>\n      <td>4.587071e+02</td>\n      <td>63.968125</td>\n      <td>3.929532e+01</td>\n    </tr>\n    <tr>\n      <th>GBDT_T_12</th>\n      <td>9.563871e-01</td>\n      <td>1.636085e+02</td>\n      <td>8.811387e+04</td>\n      <td>2.968398e+02</td>\n      <td>5.140306e+01</td>\n      <td>35.449737</td>\n      <td>7.987547e+01</td>\n    </tr>\n    <tr>\n      <th>AdaBoost_T_12</th>\n      <td>8.824441e-01</td>\n      <td>3.885201e+02</td>\n      <td>2.375054e+05</td>\n      <td>4.873453e+02</td>\n      <td>3.206372e+02</td>\n      <td>72.501307</td>\n      <td>7.245316e+01</td>\n    </tr>\n    <tr>\n      <th>RandomForest_T_12</th>\n      <td>9.478674e-01</td>\n      <td>1.712937e+02</td>\n      <td>1.053267e+05</td>\n      <td>3.245407e+02</td>\n      <td>4.325115e+01</td>\n      <td>28.243059</td>\n      <td>8.033578e+01</td>\n    </tr>\n    <tr>\n      <th>SVR_T_12</th>\n      <td>9.432010e-01</td>\n      <td>1.851871e+02</td>\n      <td>1.147545e+05</td>\n      <td>3.387544e+02</td>\n      <td>1.709560e+02</td>\n      <td>44.859362</td>\n      <td>1.638805e+03</td>\n    </tr>\n    <tr>\n      <th>MLP_T_12</th>\n      <td>9.079251e-01</td>\n      <td>2.020799e+02</td>\n      <td>1.860246e+05</td>\n      <td>4.313056e+02</td>\n      <td>8.593241e+01</td>\n      <td>42.846015</td>\n      <td>2.928600e+03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "目的変数名: T_1635, 目的変数のスケール: standard_scale\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                             r2_score     mae_score     mse_score  \\\nLinearRegression_T_1635 -1.484467e+17  5.577960e+07  1.215809e+17   \nRidge_T_1635             9.984124e-01  1.782878e-02  1.300254e-03   \nLasso_T_1635             9.580077e-01  7.736385e-02  3.439252e-02   \nElasticNet_T_1635        9.784629e-01  4.470184e-02  1.763933e-02   \nGBDT_T_1635              9.685147e-01  2.665136e-02  2.578712e-02   \nAdaBoost_T_1635          9.465113e-01  4.001407e-02  4.380829e-02   \nRandomForest_T_1635      9.779535e-01  2.209518e-02  1.805651e-02   \nSVR_T_1635               6.324454e-01  1.606127e-01  3.010346e-01   \nMLP_T_1635               5.154646e-01  1.030640e-01  3.968442e-01   \n\n                           rmse_score    mape_score  smape_score  \\\nLinearRegression_T_1635  3.486845e+08  2.596167e+10    12.668868   \nRidge_T_1635             3.605904e-02  7.895623e+00     8.139172   \nLasso_T_1635             1.854522e-01  2.760573e+01    29.844472   \nElasticNet_T_1635        1.328131e-01  1.701426e+01    19.849819   \nGBDT_T_1635              1.605837e-01  1.269135e+00     1.416722   \nAdaBoost_T_1635          2.093043e-01  6.352474e+00     7.746147   \nRandomForest_T_1635      1.343745e-01  1.936416e+00     2.582571   \nSVR_T_1635               5.486662e-01  8.256782e+01    49.621118   \nMLP_T_1635               6.299558e-01  3.718891e+01    10.998123   \n\n                         max_error_rate  \nLinearRegression_T_1635    4.746384e+10  \nRidge_T_1635               3.953388e+00  \nLasso_T_1635               3.896073e+01  \nElasticNet_T_1635          3.573989e+01  \nGBDT_T_1635                1.699783e+01  \nAdaBoost_T_1635            2.573165e+01  \nRandomForest_T_1635        1.653892e+01  \nSVR_T_1635                 6.685185e+01  \nMLP_T_1635                 8.627822e+01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r2_score</th>\n      <th>mae_score</th>\n      <th>mse_score</th>\n      <th>rmse_score</th>\n      <th>mape_score</th>\n      <th>smape_score</th>\n      <th>max_error_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LinearRegression_T_1635</th>\n      <td>-1.484467e+17</td>\n      <td>5.577960e+07</td>\n      <td>1.215809e+17</td>\n      <td>3.486845e+08</td>\n      <td>2.596167e+10</td>\n      <td>12.668868</td>\n      <td>4.746384e+10</td>\n    </tr>\n    <tr>\n      <th>Ridge_T_1635</th>\n      <td>9.984124e-01</td>\n      <td>1.782878e-02</td>\n      <td>1.300254e-03</td>\n      <td>3.605904e-02</td>\n      <td>7.895623e+00</td>\n      <td>8.139172</td>\n      <td>3.953388e+00</td>\n    </tr>\n    <tr>\n      <th>Lasso_T_1635</th>\n      <td>9.580077e-01</td>\n      <td>7.736385e-02</td>\n      <td>3.439252e-02</td>\n      <td>1.854522e-01</td>\n      <td>2.760573e+01</td>\n      <td>29.844472</td>\n      <td>3.896073e+01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet_T_1635</th>\n      <td>9.784629e-01</td>\n      <td>4.470184e-02</td>\n      <td>1.763933e-02</td>\n      <td>1.328131e-01</td>\n      <td>1.701426e+01</td>\n      <td>19.849819</td>\n      <td>3.573989e+01</td>\n    </tr>\n    <tr>\n      <th>GBDT_T_1635</th>\n      <td>9.685147e-01</td>\n      <td>2.665136e-02</td>\n      <td>2.578712e-02</td>\n      <td>1.605837e-01</td>\n      <td>1.269135e+00</td>\n      <td>1.416722</td>\n      <td>1.699783e+01</td>\n    </tr>\n    <tr>\n      <th>AdaBoost_T_1635</th>\n      <td>9.465113e-01</td>\n      <td>4.001407e-02</td>\n      <td>4.380829e-02</td>\n      <td>2.093043e-01</td>\n      <td>6.352474e+00</td>\n      <td>7.746147</td>\n      <td>2.573165e+01</td>\n    </tr>\n    <tr>\n      <th>RandomForest_T_1635</th>\n      <td>9.779535e-01</td>\n      <td>2.209518e-02</td>\n      <td>1.805651e-02</td>\n      <td>1.343745e-01</td>\n      <td>1.936416e+00</td>\n      <td>2.582571</td>\n      <td>1.653892e+01</td>\n    </tr>\n    <tr>\n      <th>SVR_T_1635</th>\n      <td>6.324454e-01</td>\n      <td>1.606127e-01</td>\n      <td>3.010346e-01</td>\n      <td>5.486662e-01</td>\n      <td>8.256782e+01</td>\n      <td>49.621118</td>\n      <td>6.685185e+01</td>\n    </tr>\n    <tr>\n      <th>MLP_T_1635</th>\n      <td>5.154646e-01</td>\n      <td>1.030640e-01</td>\n      <td>3.968442e-01</td>\n      <td>6.299558e-01</td>\n      <td>3.718891e+01</td>\n      <td>10.998123</td>\n      <td>8.627822e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "目的変数名: T_1635, 目的変数のスケール: raw_scale\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                             r2_score     mae_score     mse_score  \\\nLinearRegression_T_1635 -1.484467e+17  1.660604e+10  1.077573e+22   \nRidge_T_1635             9.984124e-01  5.307772e+00  1.152417e+02   \nLasso_T_1635             9.580077e-01  2.303185e+01  3.048213e+03   \nElasticNet_T_1635        9.784629e-01  1.330810e+01  1.563375e+03   \nGBDT_T_1635              9.685147e-01  7.934327e+00  2.285515e+03   \nAdaBoost_T_1635          9.465113e-01  1.191251e+01  3.882734e+03   \nRandomForest_T_1635      9.779535e-01  6.577915e+00  1.600351e+03   \nSVR_T_1635               6.324454e-01  4.781570e+01  2.668073e+04   \nMLP_T_1635               5.154646e-01  3.068298e+01  3.517235e+04   \n\n                           rmse_score    mape_score  smape_score  \\\nLinearRegression_T_1635  1.038062e+11  5.648290e+09    12.698954   \nRidge_T_1635             1.073507e+01  2.429073e+01    24.660169   \nLasso_T_1635             5.521062e+01  8.661920e+01    59.033438   \nElasticNet_T_1635        3.953954e+01  4.797698e+01    37.166698   \nGBDT_T_1635              4.780706e+01  1.050997e+00     1.107243   \nAdaBoost_T_1635          6.231159e+01  1.349432e+01    13.504054   \nRandomForest_T_1635      4.000438e+01  8.137869e-01     0.836168   \nSVR_T_1635               1.633424e+02  1.397734e+02    81.470853   \nMLP_T_1635               1.875429e+02  2.986621e+01    25.917668   \n\n                         max_error_rate  \nLinearRegression_T_1635    4.577525e+10  \nRidge_T_1635               3.812742e+00  \nLasso_T_1635               3.601640e+01  \nElasticNet_T_1635          3.303897e+01  \nGBDT_T_1635                1.639311e+01  \nAdaBoost_T_1635            2.481622e+01  \nRandomForest_T_1635        1.595053e+01  \nSVR_T_1635                 6.447351e+01  \nMLP_T_1635                 8.320876e+01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>r2_score</th>\n      <th>mae_score</th>\n      <th>mse_score</th>\n      <th>rmse_score</th>\n      <th>mape_score</th>\n      <th>smape_score</th>\n      <th>max_error_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LinearRegression_T_1635</th>\n      <td>-1.484467e+17</td>\n      <td>1.660604e+10</td>\n      <td>1.077573e+22</td>\n      <td>1.038062e+11</td>\n      <td>5.648290e+09</td>\n      <td>12.698954</td>\n      <td>4.577525e+10</td>\n    </tr>\n    <tr>\n      <th>Ridge_T_1635</th>\n      <td>9.984124e-01</td>\n      <td>5.307772e+00</td>\n      <td>1.152417e+02</td>\n      <td>1.073507e+01</td>\n      <td>2.429073e+01</td>\n      <td>24.660169</td>\n      <td>3.812742e+00</td>\n    </tr>\n    <tr>\n      <th>Lasso_T_1635</th>\n      <td>9.580077e-01</td>\n      <td>2.303185e+01</td>\n      <td>3.048213e+03</td>\n      <td>5.521062e+01</td>\n      <td>8.661920e+01</td>\n      <td>59.033438</td>\n      <td>3.601640e+01</td>\n    </tr>\n    <tr>\n      <th>ElasticNet_T_1635</th>\n      <td>9.784629e-01</td>\n      <td>1.330810e+01</td>\n      <td>1.563375e+03</td>\n      <td>3.953954e+01</td>\n      <td>4.797698e+01</td>\n      <td>37.166698</td>\n      <td>3.303897e+01</td>\n    </tr>\n    <tr>\n      <th>GBDT_T_1635</th>\n      <td>9.685147e-01</td>\n      <td>7.934327e+00</td>\n      <td>2.285515e+03</td>\n      <td>4.780706e+01</td>\n      <td>1.050997e+00</td>\n      <td>1.107243</td>\n      <td>1.639311e+01</td>\n    </tr>\n    <tr>\n      <th>AdaBoost_T_1635</th>\n      <td>9.465113e-01</td>\n      <td>1.191251e+01</td>\n      <td>3.882734e+03</td>\n      <td>6.231159e+01</td>\n      <td>1.349432e+01</td>\n      <td>13.504054</td>\n      <td>2.481622e+01</td>\n    </tr>\n    <tr>\n      <th>RandomForest_T_1635</th>\n      <td>9.779535e-01</td>\n      <td>6.577915e+00</td>\n      <td>1.600351e+03</td>\n      <td>4.000438e+01</td>\n      <td>8.137869e-01</td>\n      <td>0.836168</td>\n      <td>1.595053e+01</td>\n    </tr>\n    <tr>\n      <th>SVR_T_1635</th>\n      <td>6.324454e-01</td>\n      <td>4.781570e+01</td>\n      <td>2.668073e+04</td>\n      <td>1.633424e+02</td>\n      <td>1.397734e+02</td>\n      <td>81.470853</td>\n      <td>6.447351e+01</td>\n    </tr>\n    <tr>\n      <th>MLP_T_1635</th>\n      <td>5.154646e-01</td>\n      <td>3.068298e+01</td>\n      <td>3.517235e+04</td>\n      <td>1.875429e+02</td>\n      <td>2.986621e+01</td>\n      <td>25.917668</td>\n      <td>8.320876e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# モデルの評価指標の一覧を算出する。\n",
    "evaluation_scores(output_dir, targets, models, folder_name)"
   ]
  },
  {
   "source": [
    "# Part:10 学習した予測モデルの特徴を分析"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_features(output_dir_path, features, targets, folder_name):\n",
    "    \"\"\"回帰係数/変数重要度を取得・描画する\n",
    "\n",
    "    各目的変数に対して各種モデルの回帰係数 or 変数重要度の値を取得及び描画する。\n",
    "    「models/folder_name/feature_analysis_plot/'目的変数名'」配下に\n",
    "    「folder_name_'モデル名'_'目的変数名'.csv」、\n",
    "    「folder_name_'モデル名'_'目的変数名'.png」という名称で出力する\n",
    "      \n",
    "    - 予測モデルが決定木系('AdaBoost', 'GBDT', 'RandomForest')\n",
    "      ⇒ 変数重要度を取得・描画する。\n",
    "    \n",
    "    - 予測モデルが線形回帰系('ElasticNet', 'Lasso', 'LinearRegression', 'Ridge')\n",
    "      ⇒ 回帰係数を取得・描画する。\n",
    "    \n",
    "    - 予測モデルが上記以外('SVR', 'MLP')\n",
    "      ⇒ 何もしない.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        features (list): 説明変数のリスト.\n",
    "        targets (list): 目的変数のリスト.\n",
    "        folder_name (str): 今回の実験の各種記録を行っているフォルダの名前.\n",
    "\n",
    "    \"\"\"\n",
    "    linear_regression_alg = {'El', 'La', 'Li', 'Ri'}\n",
    "    decesion_tree_alg = {'Ad', 'GB', 'Ra'}\n",
    "\n",
    "    for target in targets:\n",
    "        # 特徴重要度(線形回帰)のグラフを格納するパスを指定、フォルダを作成\n",
    "        feature_analysis_target_path = join(output_dir_path, f'feature_analysis_plot/{target}')\n",
    "        if not os.path.exists(feature_analysis_target_path):\n",
    "            os.makedirs(feature_analysis_target_path)\n",
    "        \n",
    "        trained_file_path = join(output_dir_path, f'trained_model/{target}')\n",
    "        trained_files = os.listdir(trained_file_path)\n",
    "        for trained_file in trained_files:\n",
    "            with open(join(trained_file_path, trained_file), 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            file_name = f\"{folder_name}_{trained_file[13:-7]}\"\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # 線形回帰系のアルゴリズムは0でない値を持つ回帰係数をを上位10個出力する\n",
    "            if trained_file[13:15] in linear_regression_alg:\n",
    "                model_coefficient = (pd.DataFrame(model.coef_, index=features, columns=['coef'])\n",
    "                                       .query('coef != 0'))\n",
    "\n",
    "                model_coefficient.sort_values('coef', key=lambda col: abs(col), ascending=False, inplace=True)\n",
    "                model_coefficient['is_plus'] = model_coefficient['coef'] >= 0\n",
    "                (model_coefficient.drop('is_plus', axis=1)\n",
    "                                  .to_csv(join(feature_analysis_target_path, f'{file_name}.csv'),\n",
    "                                          encoding='utf-8_sig'))\n",
    "                \n",
    "                plot_data = model_coefficient.head(10).sort_values('coef', key=lambda col: abs(col))\n",
    "                plot_data['coef'].plot(kind='barh', ax=ax, legend=False,\n",
    "                                       title=f'coef: {trained_file[13:-7]}',\n",
    "                                       color=plot_data['is_plus'].map({True: '#1f77b4',  # 青色\n",
    "                                                                       False: '#d62728'}))  # 赤色\n",
    "            # 決定木系のアルゴリズムは特徴重要度を上位10個を出力する\n",
    "            elif trained_file[13:15] in decesion_tree_alg:\n",
    "                model_feature_importances = (pd.DataFrame(model.feature_importances_.T,\n",
    "                                                         index=features, columns=['feature_importances'])\n",
    "                                               .sort_values('feature_importances', ascending=False))\n",
    "                model_feature_importances.to_csv(join(feature_analysis_target_path, f'{file_name}.csv'),\n",
    "                                                 encoding='utf-8_sig')\n",
    "                \n",
    "                (model_feature_importances[0:10].sort_values('feature_importances')\n",
    "                                                .plot(kind='barh', ax=ax, legend=False,\n",
    "                                                      title=f'feature inportance: {trained_file[13:-7]}',\n",
    "                                                      color='#1f77b4'))\n",
    "            else:\n",
    "                plt.close()\n",
    "                continue\n",
    "\n",
    "            plt.grid()\n",
    "            plt.savefig(join(feature_analysis_target_path, f'{file_name}.png'),\n",
    "                        facecolor='white', bbox_inches='tight')\n",
    "            \n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル毎に変数需要度・回帰係数を算出・描画します。\n",
    "models_features(output_dir, features, targets, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(output_dir_path, targets, folder_name):\n",
    "    \"\"\"実測値と予測結果の残差を可視化する\n",
    "\n",
    "    Args:\n",
    "        output_dir_path (str): アウトプット先のフォルダパス.\n",
    "        target (str): 目的変数の名前のリスト.\n",
    "        folder_name (str): 今回の実験の各種記録を行っているフォルダの名前.\n",
    "\n",
    "    \"\"\"\n",
    "    for target in targets:\n",
    "        # 画像を格納するフォルダ\n",
    "        path_plot_dict = {'standard_scale': f'{output_dir_path}/yy_residual_plots/standard_scale/{target}',\n",
    "                          'raw_scale': f'{output_dir_path}/yy_residual_plots/raw_scale/{target}'}\n",
    "        for path in path_plot_dict.values():\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        # 予測結果を格納しているフォルダ\n",
    "        path_predicts_dict = {'standard_scale': f'{output_dir_path}/predicts/standard_scale/{target}',\n",
    "                              'raw_scale': f'{output_dir_path}/predicts/raw_scale/{target}'}\n",
    "\n",
    "        # 目的変数の値を格納しているフォルダ\n",
    "        path_observes_dict = {'standard_scale': f'{output_dir_path}/observes/standard_scale/{target}',\n",
    "                              'raw_scale': f'{output_dir_path}/observes/raw_scale/{target}'}\n",
    "\n",
    "        for scale_name in ['standard_scale', 'raw_scale']:\n",
    "            for predict_file in os.listdir(path_predicts_dict[scale_name]):\n",
    "                obs = pd.read_csv(join(path_observes_dict[scale_name], f'{folder_name}_{target}.csv'),\n",
    "                                  index_col=0)[target]\n",
    "                pred = pd.read_csv(join(path_predicts_dict[scale_name], predict_file),\n",
    "                                   index_col=0)['predicts']\n",
    "                \n",
    "                df_score = pd.DataFrame({'Observed': obs, 'Predicted': pred},\n",
    "                                        index=pred.index)\n",
    "                # 予測値が無限大になるサンプルを削除する.\n",
    "                df_score = df_score.loc[df_score['Predicted'] != np.inf, :]\n",
    "\n",
    "                df_score['residual'] = df_score['Predicted'] - df_score['Observed']\n",
    "\n",
    "                df_score['Cell_type'] = df_score.index\n",
    "                df_score.index = df_score.index.str.split('_', expand=True)\n",
    "                df_score.index.names = ['type', 'sample_no']\n",
    "                df_score.reset_index(inplace=True)\n",
    "\n",
    "                # yyプロットと残差プロットを描画する.\n",
    "                fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(13, 4))\n",
    "                cell_types = df_score['type']\n",
    "                hue_order = list(str(i) for i in range(int(cell_types.min()), int(cell_types.max())+1))\n",
    "                plot_args = dict(hue='type', hue_order=hue_order, data=df_score)\n",
    "                sns.scatterplot(x='Observed', y='Predicted', ax=ax1, legend=False, **plot_args)\n",
    "                sns.scatterplot(x=df_score.index, y='residual', ax=ax2, **plot_args)\n",
    "\n",
    "                lim_max = df_score[['Observed', 'Predicted']].max().max()\n",
    "                lim_min = df_score[['Observed', 'Predicted']].min().min()\n",
    "\n",
    "                if abs(df_score['Observed']).max() > abs(df_score['Predicted']).max():\n",
    "                    ticks = ax1.get_xticks()\n",
    "                else:\n",
    "                    ticks = ax1.get_yticks()\n",
    "\n",
    "                if (ticks[-1] - ticks[-2]) < 0.5:\n",
    "                    buffer = 0.05\n",
    "                else:\n",
    "                    buffer = 0.5\n",
    "\n",
    "                if (ticks[-1] - ticks[-2]) < 0.1:\n",
    "                    ax1.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "                ax1.set_yticks(ticks)\n",
    "                ax1.set_xticks(ticks)\n",
    "\n",
    "                ax1.set_xlim([lim_min - buffer, lim_max + buffer])\n",
    "                ax1.set_ylim([lim_min - buffer, lim_max + buffer])\n",
    "                ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "                ylim_min, ylim_max = ax2.get_ylim()\n",
    "                if abs(df_score['residual'].min()) < abs(df_score['residual'].max()):\n",
    "                    ax2.set_ylim([-1*ylim_max, ylim_max])\n",
    "                else:\n",
    "                    ax2.set_ylim([ylim_min, -1 * ylim_min])\n",
    "\n",
    "                ax1.set_ylabel('Predicted')\n",
    "                ax1.set_xlabel('Observed')\n",
    "                ax2.set_ylabel('residual')\n",
    "                ax1.set_title(f'Observed-Predicted Plot\\n{predict_file[13:-7]}: {scale_name}')\n",
    "                ax2.set_title(f'residual plot\\n{predict_file[13:-7]}: {scale_name}')\n",
    "\n",
    "                for ax in [ax1, ax2]:\n",
    "                    ax.grid()\n",
    "                ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Cell_type')\n",
    "\n",
    "                plt.savefig(join(path_plot_dict[scale_name], f'{folder_name}_{predict_file[13:-7]}.png'),\n",
    "                            bbox_inches=\"tight\", dpi=100)\n",
    "                plt.close()\n",
    "\n",
    "                (df_score.loc[:, ['Cell_type', 'Observed', 'Predicted', 'residual']]\n",
    "                         .to_csv(join(join(path_plot_dict[scale_name],\n",
    "                                 f'{folder_name}_{predict_file[13:-7]}.csv'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yyプロット、残差プロットを描画します。\n",
    "residual_plot(output_dir, targets, folder_name)"
   ]
  }
 ]
}